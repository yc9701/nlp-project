#!/usr/bin/env python3

import sys, os
from os import makedirs, system, chdir, getcwd
from os.path import basename, exists, join, expanduser
from contextlib import contextmanager
from six.moves.urllib.request import FancyURLopener
from six.moves.urllib.parse import urlparse

class suppress_output: 
    def __init__(self,suppress_stdout=False,suppress_stderr=False): 
        self.suppress_stdout = suppress_stdout 
        self.suppress_stderr = suppress_stderr 
        self._stdout = None 
        self._stderr = None
    def __enter__(self): 
        devnull = open(os.devnull, "w") 
        if self.suppress_stdout: 
            self._stdout = sys.stdout 
            sys.stdout = devnull        
        if self.suppress_stderr: 
            self._stderr = sys.stderr 
            sys.stderr = devnull 
    def __exit__(self, *args): 
        if self.suppress_stdout: 
            sys.stdout = self._stdout 
        if self.suppress_stderr: 
            sys.stderr = self._stderr

DEFAULT_MODELS_DIR = '~/.local/share/bllipparser'

class ModelInfo:
    """Information about a unified BLLIP Parser model. Interface and
    fields subject to change."""
    def __init__(self, model_desc, url, uncompressed_size):
        """uncompressed_size is approximate size in megabytes."""
        self.model_desc = model_desc
        self.url = url
        self.uncompressed_size = uncompressed_size
    def __str__(self):
        return "%s [%sMB]" % (self.model_desc, self.uncompressed_size)

models = {
    'WSJ-PTB3': ModelInfo('Wall Street Journal corpus from Penn Treebank, version 3',
                          'https://www.dropbox.com/s/ulcfs7my1ifriuu/BLLIP-WSJ-PTB3.tar.bz2?dl=1', 55)
}

def download_and_install_model(model_name, models_directory=None,
                               verbose=False):
    """Downloads and installs models to a specific directory. Models can
    be specified by simple names (use list_models() for a list of known
    models) or a URL. If the model is already installed as a subdirectory
    of models_directory, it won't download it again. By default,
    models will be downloaded to "~/.local/share/bllipparser". Setting
    verbose=True will print extra details and download progress. Returns
    the path to the new model."""

    if model_name.lower().startswith('http'):
        parsed_url = urlparse(model_name)
        model_url = model_name
        model_name = basename(parsed_url.path).split('.')[0]
    elif model_name in models:
        model_url = models[model_name].url
    else:
        raise UnknownParserModel(model_name)

    models_directory = models_directory or DEFAULT_MODELS_DIR
    models_directory = expanduser(models_directory)

    output_path = join(models_directory, model_name)

    if exists(output_path):
        return output_path

    if verbose:
        def status_func(blocks, block_size, total_size):
            amount_downloaded = blocks * block_size
            if total_size == -1:
                sys.stdout.write('Downloaded %s\r' % amount_downloaded)
            else:
                percent_downloaded = 100 * amount_downloaded / total_size
                size = amount_downloaded / (1024 ** 2)
                sys.stdout.write('Downloaded %.1f%% (%.1f MB)\r' %
                                 (percent_downloaded, size))
    else:
        status_func = None

    # needed since 404s, etc. aren't handled otherwise
    class ErrorAwareOpener(FancyURLopener):
        def http_error_default(self, url, fp, errcode, errmsg, headers):
            raise SystemExit("Error downloading model (%s %s)" %
                             (errcode, errmsg))

    opener = ErrorAwareOpener()
    downloaded_filename, headers = opener.retrieve(model_url,
                                                   reporthook=status_func)

    try:
        makedirs(output_path)
    except OSError as ose:
        if ose.errno != 17:
            raise

    orig_path = getcwd()
    chdir(output_path)
    # by convention, all models are currently in tar.bz2 format
    # we may want to generalize this code later
    assert downloaded_filename.lower().endswith('.bz2')
    # command = 'tar xvjf %s >/dev/null 2>&1' % downloaded_filename
    command = 'tar xvjf %s' % downloaded_filename
    system(command)
    chdir(orig_path)

    return output_path

# with suppress_output(suppress_stdout=True,suppress_stderr=True):
import nltk
import string
from bllipparser import RerankingParser
# download_and_install_model('WSJ-PTB3', verbose=False)
rrp = RerankingParser.fetch_and_load('WSJ-PTB3', verbose=False)
import spacy
from spacy import displacy
import en_core_web_md
nlp = en_core_web_md.load()
import preprocessing as preprocess
from nltk.corpus import wordnet
import statistics
import numpy
import random
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')

auxi_verbs = ["am", "is", "are", "was", "were", "have", "has", 
    "had", "do", "does", "did", "will", "would", "shall", "should", 
    "may", "might", "must", "can", "could"]
negate_words = ["not", "never", "no", "n't"]
q_begins = ["Were", "Might", "Was", "Is", "Did", "Had", "Has", "Could"]

class Generator():
    def __init__(self, preprocessed_data, tagged_data, ner_data):
        # list of sentences, each sentence is a list of words as strings
        self.data = preprocessed_data
        # list of list of (word, POS tag) tuples
        self.tagged = tagged_data
        # ner data in spacy format
        self.ner = ner_data
        # set of all the questions we generate (questions are in the form of strings)
        self.noQuestions = set()
        self.whQuestions = set()
        self.yesQuestions = set()

    # capitalize first letter in the sentence
    def capitalizeSent(self, sentence):
        firstLetter = sentence[0].upper()
        restOfSent = sentence[1:len(sentence)]
        return firstLetter + restOfSent

    # Returns the preceeding part of a parse_tree
    # NOTE: Includes a trailing whitespace if non-empty
    def printPhraseBefore(self, parse_tree, index):
        phrase = ""
        preceeding_space = False
        if index == 0:
            return phrase
        for i in range(index):
            tree_phrase = parse_tree.__getitem__(i).tokens()
            for token in tree_phrase:
                if token == "-LRB-":
                    if preceeding_space:
                        phrase += " "
                    phrase += "("
                    preceeding_space = False
                elif token == "-RRB-":
                    phrase += ")"
                    preceeding_space = True
                elif token == "-LSB-":
                    if preceeding_space:
                        phrase += " "
                    phrase += "["
                    preceeding_space = False
                elif token == "-RSB-":
                    phrase += "]"
                    preceeding_space = True
                elif token in string.punctuation:
                    phrase += token
                    preceeding_space = True
                else:
                    if preceeding_space:
                        phrase += " "
                    phrase += token
                    preceeding_space = True
        return phrase

    # Returns the successive part of a parse_tree
    # NOTE: Includes a leading whitespace if non-empty
    def printPhraseAfter(self, parse_tree, index):
        phrase = ""
        preceeding_space = False
        if index == (len(parse_tree)-1):
            return phrase 
        for i in range(index + 1, len(parse_tree)):
            tree_phrase = parse_tree.__getitem__(i).tokens()
            for token  in tree_phrase:
                if token == "-LRB-":
                    if preceeding_space:
                        phrase += " "
                    phrase += "("
                    preceeding_space = False
                elif token == "-RRB-":
                    phrase += ")"
                    preceeding_space = True
                elif token in string.punctuation:
                    phrase += token
                    preceeding_space = True
                else:
                    if preceeding_space:
                        phrase += " "
                    phrase += token
                    preceeding_space = True
        return phrase
    
    # Recursive helper for getSubstitutions, loops through a non-NP phrase
    def getSubRecursive(self, parse_tree):
        # Initialize an empty set
        result = set()
        # Loop through each labeled part of the phrase
        for i in range(len(parse_tree)):
            # Recursive call to search for possible substitutions
            poss_subs = self.getSubstitutions(parse_tree, i)
            # For each substitution, make that change in our phrase
            for sub in poss_subs:
                prev_phrase = self.printPhraseBefore(parse_tree, i)
                if prev_phrase != "":
                    prev_phrase += " "
                next_phrase = self.printPhraseAfter(parse_tree, i)
                if next_phrase != "":
                    next_phrase = " " + next_phrase
                comb_phrase = prev_phrase + sub + next_phrase
                # Add the changed phrase to result set
                result.add(comb_phrase)
        # Return (possibly empty) set of all(?) possible changes
        return result

    # Returns a set of phrases, where each phrase makes a substitution
    # of a NP with an appropriate WH word
    # If no NP found within indexed subtree, returns empty string
    # Hope to eventually make this recursive:
    # NOTE: current implementation only checks top level tree,
    # TODO: implement recursion to check all levels of the tree for replacable NP
    def getSubstitutions(self, parse_tree, index):
        # Get a subtree, check its label
        subtree = parse_tree.__getitem__(index)
        # (Replacable) noun phrase found
        if subtree.label == 'NP':
            # Find the entity type of the NP
            with suppress_output(suppress_stdout=True,suppress_stderr=True):
                doc = nlp(" ".join(subtree.tokens()))
            # Hopefully these top level NP are fairly simple
            # Guess every relevant WH word for substitution
            wh_word = set()
            for ent in doc.ents:
                if ent.label_ == 'PERSON':
                    # WH-word = who
                    wh_word.add("who")
                elif ent.label_ == "DATE":
                    # WH-word = when
                    wh_word.add("when")
                elif ent.label == "TIME":
                    # WH-word = when
                    wh_word.add("when")
                elif ent.label_ == 'LOCATION':
                    # WH-word = where
                    wh_word.add("where")
                else:
                    # WH-word = what
                    wh_word.add("what")
                # NOTE: Read note in generateWHQ
            return wh_word
        # Recurse and try to look for inner substitutions
        else:
            # If length is 1, there can be no subsitutions
            if len(subtree) <= 1:
                return set()
            # Otherwise, phrase can be further broken down
            else:
                return self.getSubRecursive(subtree)

    # Get WH questions
    def generateWHQ(self):
        # Loop through sentences
        for sentence in self.data:
            # Construct parse_tree
            scored_parse_obj = rrp.parse(sentence)
            # Loop through the top layer of sentence
            for i in range(scored_parse_obj[0].ptb_parse.__len__()):
                # Get possible substitution for a NP in this index
                sub_phrases = self.getSubstitutions(scored_parse_obj[0].ptb_parse, i)
                # Check for successful replacement
                for sub in sub_phrases:
                    # Get the rest of the sentence and construct question
                    prev_phrase = self.printPhraseBefore(scored_parse_obj[0].ptb_parse, i)
                    if prev_phrase != "":
                        prev_phrase += " "
                    next_phrase = self.printPhraseAfter(scored_parse_obj[0].ptb_parse, i)
                    if next_phrase != "":
                        next_phrase = " " + next_phrase
                    question = prev_phrase + sub + next_phrase
                    question = question[0:(len(question)-2)] + "?"

                    # capitalize first letter in sentence
                    question = self.capitalizeSent(question)
                    self.whQuestions.add(question)

        # NOTE: for "why" and "how", we probably
        # need a way besides using NER. for
        # example detecting words like "because" or "due to"
    
    def sentence_yes_questions(self, sentence_init):
        result = []
        if (sentence_init[0] == "However"):
            sentence = sentence_init[2:]
        else:
            sentence = sentence_init
        for i in range(len(sentence)):
            if ((sentence[i] in negate_words) or ("n't" in sentence[i]) or (sentence[i] == ",")):
                return ""
            if (sentence[i] in auxi_verbs):
                for j in range(i):
                    result.append(sentence[j])
                if (i < len(sentence)-1):
                    for k in range(i+1, len(sentence)-1):
                        result.append(sentence[k])
        
                result.insert(0, sentence[i][0].swapcase() + sentence[i][1:])
                result2 = ""
                for word in result:
                    result2 += " "+word
                result2 += "?"
                return result2[1:]
        return ""

    def generateYesQ(self):
        for sentence in self.data:
            question = self.sentence_yes_questions(sentence)
            if not(question == ""):
                self.yesQuestions.add(question)

    def antonymSentence(self, sentence):
        result = []
        no_ant_result = []
        ant_ind = []
        questions = []
        for i in range(len(sentence)):
            result.append(sentence[i][0])
            no_ant_result.append(sentence[i][0])
            if sentence[i][0] in auxi_verbs:
                result.pop()
                no_ant_result.pop()
                for k in range(i+1, len(sentence)):
                    antonyms = set()
                    no_ant_result.append(sentence[k][0])
                    # if punctuation, pass
                    if sentence[k][0] == ';':
                        break

                    # check if there's an adjective that can be negated
                    if sentence[k][1] == 'JJ':
                        for syn in wordnet.synsets(sentence[k][0]):
                            for lemma in syn.lemmas():
                                if lemma.antonyms():
                                    antonyms.add(lemma.antonyms()[0].name())
                    # for now, just add the first antonym; but later, should be able to add all antonyms
                    if len(antonyms) > 1:
                        result.append(antonyms)
                        ant_ind.append(k)
                    elif len(antonyms) == 1:
                        result.append(antonyms)
                    else:
                        result.append(sentence[k][0])
                result.insert(0, sentence[i][0][0].swapcase() + sentence[i][0][1:])
                result.append("?")
                no_ant_result.insert(0, sentence[i][0][0].swapcase() + sentence[i][0][1:])
                no_ant_result.append("?")
                break

        questions.append(no_ant_result)

        for i in ant_ind:
            antonyms = result[i] # list of antonyms at that ind
            temp_questions = []
            for question in questions:
                if question == []:
                    continue
                for k in antonyms:
                    new_question = question
                    new_question[i] = k
            
            questions.append(temp_questions)

        return questions

    def makeString(self, question): # question : list
        output = ""
        paren = False # if prev is an open paren
        for i in question:
            if not i in string.punctuation:
                if not paren:
                    output += " "
                else:
                    paren = False
                output += i
            else:
                if i == "(":
                    output += " "
                    paren = True
                if i == ".":
                    output += "?"
                    break
                output += i

        return output[1:]

    def generateNoQ(self):
        for sentence in self.tagged:
            result = []
            for i in range(len(sentence)):
                if sentence[i][0] in auxi_verbs:
                    for j in range(i):
                        result.append(sentence[j][0])

                    result.append("not")

                    if i < len(sentence) - 1:
                        for k in range(i+1, len(sentence)):
                            if sentence[k][0] == ';':
                                break
                            result.append(sentence[k][0])
                
                if sentence[i][0] == ";":
                    break
                    
            result.insert(0, sentence[i][0][0].swapcase() + sentence[i][0][1:])

            if result[0] == '.':
                del result[0]
            if not result == []:
                if result[len(result)-1] == '.':
                    del result[-1]
                if result[len(result)-2] == '.':
                    del result[len(result)-2]
                if not result[0] in q_begins:
                    result[0] = result[0].lower()
                    result.insert(0,"Did")
                else:
                    result[1] = result[1].lower()
                if not len(result) == 1:
                    result.append("?")
                    self.noQuestions.add(self.makeString(result))

            antonym_questions = self.antonymSentence(sentence)
            for i in antonym_questions:
                temp = i
                if temp == []: continue
                if temp[0] == '.':
                    del temp[0]
                if temp[len(temp)-2] == '.':
                    del temp[len(temp)-2]
                if not temp[len(temp)-1] == '?':
                    temp.append('?')
                if not temp[0] in q_begins:
                    temp[0] = temp[0].lower()
                    temp.insert(0,"Did")
                self.noQuestions.add(self.makeString(temp))

    # returns a list of question lengths, with corresponding indices
    def getLengths(self, questions):
        lengths = []
        for sentence in questions:
            lengths.append(len(sentence))
        return lengths

    # find number of pronouns in question
    def numPronouns(self, question):
        count = 0
        tokenized = nltk.word_tokenize(question)
        tagged = nltk.pos_tag(tokenized)
        for (word,tag) in tagged:
            if tag == "PRP" or tag == "PRP$":
                count += 1
        return count

    # given a question, assigns a score of "goodness"
    # score is based off quantile with preference given to shorter questions
    def scoreQuestion(self, question, distribution):
        question_length = len(question)
        rank = 0
        offset = 1

        for i in range(len(distribution)):
            if question_length < distribution[i]:
                rank = i
                break
            rank = len(distribution)

        # bonus gives preference to shorter questions
        bonus = 0
        if rank < len(distribution)/2.0:
            bonus = 1.25
        
        # give penalty to questions with more pronouns
        penalty = 0.15 * self.numPronouns(question)
        
        score = abs(len(distribution)/2.0 - rank - offset) + bonus - penalty
        return score * random.uniform(0.5, 1) + random.uniform(-1, 1)

    # given a set/list of questions, assign a quartile distribution based on lengths
    def getDistribution(self, questions):
        lengths = self.getLengths(questions)
        return [round(q, 1) for q in numpy.percentile(lengths, numpy.arange(0, 100, 10))]

    # given a set of questions, sorts them in order of decreasing score
    def sortQuestions(self, questions):
        question_list = list(questions)
        distribution = self.getDistribution(question_list)
        question_list.sort(reverse=True,key = lambda q: self.scoreQuestion(q, distribution))
        return question_list


    def getTopNQs(self, n):
        count = 0
        sorted_wh = self.sortQuestions(self.whQuestions)
        sorted_yes = self.sortQuestions(self.yesQuestions)
        sorted_no = self.sortQuestions(self.noQuestions)

        big_count = 0
        for i in sorted_wh:
            if i.isspace(): continue # filter out empty questions
            count += 1
            print(i)
            print()
            big_count += 1
            if count == (n//2 + n%2 + (n%4)//2): break

        count = 0
        for i in sorted_yes:
            if i.isspace(): continue
            count += 1
            print(i)
            print()
            big_count += 1
            if count == n//4: break
        
        count = 0
        for i in sorted_no:
            if i.isspace(): continue
            count += 1
            print(i)
            print()
            big_count += 1
            if count == n//4: break

        print("num questions: " + str(big_count))

if __name__ == "__main__":
    # Files
    data_file = sys.argv[1]
    nquestions = int(sys.argv[2])

    data = preprocess.open_file(data_file)
    (preprocessed_data, tagged_data) = preprocess.pos_tokenize(data)
    ner_data = preprocess.ner(data)

    qGenerator = Generator(preprocessed_data, tagged_data, ner_data)
    qGenerator.generateWHQ()
    qGenerator.generateYesQ()
    qGenerator.generateNoQ()

    qGenerator.getTopNQs(nquestions)