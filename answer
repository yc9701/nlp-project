#!/usr/bin/env python3

import sys, os
from os import makedirs, system, chdir, getcwd
from os.path import basename, exists, join, expanduser
from contextlib import contextmanager
from six.moves.urllib.request import FancyURLopener
from six.moves.urllib.parse import urlparse

class suppress_output: 
    def __init__(self,suppress_stdout=False,suppress_stderr=False): 
        self.suppress_stdout = suppress_stdout 
        self.suppress_stderr = suppress_stderr 
        self._stdout = None 
        self._stderr = None
    def __enter__(self): 
        devnull = open(os.devnull, "w") 
        if self.suppress_stdout: 
            self._stdout = sys.stdout 
            sys.stdout = devnull        
        if self.suppress_stderr: 
            self._stderr = sys.stderr 
            sys.stderr = devnull 
    def __exit__(self, *args): 
        if self.suppress_stdout: 
            sys.stdout = self._stdout 
        if self.suppress_stderr: 
            sys.stderr = self._stderr

DEFAULT_MODELS_DIR = '~/.local/share/bllipparser'

class ModelInfo:
    """Information about a unified BLLIP Parser model. Interface and
    fields subject to change."""
    def __init__(self, model_desc, url, uncompressed_size):
        """uncompressed_size is approximate size in megabytes."""
        self.model_desc = model_desc
        self.url = url
        self.uncompressed_size = uncompressed_size
    def __str__(self):
        return "%s [%sMB]" % (self.model_desc, self.uncompressed_size)

models = {
    'WSJ-PTB3': ModelInfo('Wall Street Journal corpus from Penn Treebank, version 3',
                          'https://www.dropbox.com/s/ulcfs7my1ifriuu/BLLIP-WSJ-PTB3.tar.bz2?dl=1', 55)
}

def download_and_install_model(model_name, models_directory=None,
                               verbose=False):
    """Downloads and installs models to a specific directory. Models can
    be specified by simple names (use list_models() for a list of known
    models) or a URL. If the model is already installed as a subdirectory
    of models_directory, it won't download it again. By default,
    models will be downloaded to "~/.local/share/bllipparser". Setting
    verbose=True will print extra details and download progress. Returns
    the path to the new model."""

    if model_name.lower().startswith('http'):
        parsed_url = urlparse(model_name)
        model_url = model_name
        model_name = basename(parsed_url.path).split('.')[0]
    elif model_name in models:
        model_url = models[model_name].url
    else:
        raise UnknownParserModel(model_name)

    models_directory = models_directory or DEFAULT_MODELS_DIR
    models_directory = expanduser(models_directory)

    output_path = join(models_directory, model_name)

    if exists(output_path):
        return output_path

    if verbose:
        def status_func(blocks, block_size, total_size):
            amount_downloaded = blocks * block_size
            if total_size == -1:
                sys.stdout.write('Downloaded %s\r' % amount_downloaded)
            else:
                percent_downloaded = 100 * amount_downloaded / total_size
                size = amount_downloaded / (1024 ** 2)
                sys.stdout.write('Downloaded %.1f%% (%.1f MB)\r' %
                                 (percent_downloaded, size))
    else:
        status_func = None

    # needed since 404s, etc. aren't handled otherwise
    class ErrorAwareOpener(FancyURLopener):
        def http_error_default(self, url, fp, errcode, errmsg, headers):
            raise SystemExit("Error downloading model (%s %s)" %
                             (errcode, errmsg))

    opener = ErrorAwareOpener()
    downloaded_filename, headers = opener.retrieve(model_url,
                                                   reporthook=status_func)

    try:
        makedirs(output_path)
    except OSError as ose:
        if ose.errno != 17:
            raise

    orig_path = getcwd()
    chdir(output_path)
    # by convention, all models are currently in tar.bz2 format
    # we may want to generalize this code later
    assert downloaded_filename.lower().endswith('.bz2')
    # command = 'tar xvjf %s >/dev/null 2>&1' % downloaded_filename
    command = 'tar xvjf %s >/dev/null 2>&1' % downloaded_filename
    system(command)
    chdir(orig_path)

    return output_path

with suppress_output(suppress_stdout=True,suppress_stderr=True):
    import nltk
    import string
    from bllipparser import RerankingParser
    download_and_install_model('WSJ-PTB3', verbose=False)
    rrp = RerankingParser.fetch_and_load('WSJ-PTB3', verbose=False)
    import spacy
    from spacy import displacy
    import en_core_web_md
    nlp = en_core_web_md.load()
    import sys
    import preprocessing as preprocess
    from nltk.corpus import wordnet
    from nltk.corpus import stopwords
    nltk.download('stopwords')
    nltk.download('punkt')
    nltk.download('averaged_perceptron_tagger')
    nltk.download('wordnet')

auxi_verbs = ["am", "is", "are", "was", "were", "have", "has", 
    "had", "do", "does", "did", "will", "would", "shall", "should", 
    "may", "might", "must", "can", "could"]

negate_words = ["not", "never", "no", "n't"]

class Answerer():
    def __init__(self, preprocessed_data, tagged_data, vectors_data, ner_data, 
    preprocessed_questions, tagged_questions, vectors_questions, ner_questions):
        # list of sentences, each sentence is a list of words as strings
        self.data = preprocessed_data
        # list of list of (word, POS tag) tuples
        self.tagged = tagged_data
        # list of vectors of sentence meanings
        self.vectors = vectors_data
        # ner data in spacy format
        self.ner = ner_data
        # list of questions, each question is a list of words as strings
        self.questions = preprocessed_questions
        # list of list of (word, POS tag) tuples
        self.tagged_questions = tagged_questions
        # list of vectors of question meanings
        self.question_vectors = vectors_questions
        # ner questions in spacy format
        self.ner_questions = ner_questions
        # list of all the answers in order of questions asked
        # (questions are in the form of strings)
        self.answers = []
        self.wh_tags = ['WDT', 'WP', 'WRB']



    # input i: the index of the question we are identifying
    # returns True if question at self.questions[i] is a WH-question
    # returns False otherwise
    def isWHQuestion(self, i):
        for (word, tag) in self.tagged_questions[i]:
            if tag in self.wh_tags:
                return True
        return False
    
    # input sentence: the sentence for stopwords to be removed
    # returns The same sentence list but with common words removed
    def removeStopwords(self, sentence):
        common_words = set(stopwords.words('english'))
        sentence_list = nltk.word_tokenize(str(sentence))
        return nlp(" ".join(list(filter(lambda w: w.lower() not in common_words, sentence_list))))

    # finds the sentence in the document that is most similar to the ith question
    # input i: the index of question to search for
    # output: string that is most similar
    def mostSimilar(self, i):
        question = self.question_vectors[i]
        query = self.removeStopwords(question)
        candidate_index = 0
        for j in range(len(self.vectors)):
            if self.vectors[j].similarity(query) > self.vectors[candidate_index].similarity(query):
                candidate_index = j
        return self.data[candidate_index]
        
    # answers the ith question
    # given a question, parses the document looking for the sentence with the highest similarity and returns it
    # input i: the index of question to answer
    # REQUIRES: ith question is WH question
    # output: string that is the most likely answer
    def answerWHQuestion(self, i):
        # store a running tab of the index of the current most similar sentence
        sentence = self.mostSimilar(i)
        # find answer in sentence by going through each word in the sentence
        # and seeing if it's in the question
        result = ""
        for word in sentence:
            if word not in string.punctuation and word not in self.questions[i]:
                result += word + " "
        result = result[0:len(result)-1]
        return result
  
    # answers the ith question
    # input i: the index of question to answer
    # REQUIRES: ith question is Yes/no question
    # output: string that is the most likely answer
    def answerYesNoQuestion(self, i):
        # get the most similar sentence
        sentence = self.mostSimilar(i)
        question = self.questions[i]
        # answer = true
        # look at sentence
        # for each word in the sentence, see if synonym/antonym in question
        # check word in sentence and question
        # case on each being preceeded by negating words
        for j in range(len(sentence)):
            word = sentence[j]
            synonyms = set()
            antonyms = set()
            for synset in wordnet.synsets(word):
                for lemma in synset.lemmas():
                    synonyms.add(lemma.name())
                    if lemma.antonyms():
                        for anti in lemma.antonyms():
                            antonyms.add(anti.name())
            for k in range(len(question)):
                if question[k] == word or question[k] in synonyms:
                    if (j > 0 and sentence[j-1] in negate_words) and (k == 0 or (k > 0 and question[k-1] not in negate_words)):
                        return "No"
                    elif (j == 0 or (j > 0 and sentence[j-1] not in negate_words)) and (k > 0 and question[k-1] in negate_words):
                        return "No"
                elif question[k] in antonyms:
                    if (j == 0 or (j > 0 and sentence[j-1] not in negate_words)) and (k == 0 or (k > 0 and question[k-1] not in negate_words)):
                        return "No"
                    elif (j > 0 and sentence[j-1] in negate_words) and (k > 0 and question[k-1] in negate_words):
                        return "No"
        return "Yes"


            

    # answers questions in self.questions and puts answers in same order
    # in self.answers
    # output: none
    def answerQuestions(self):
        for i in range(len(self.questions)):
            if self.isWHQuestion(i):
                self.answers.append(self.answerWHQuestion(i))
            else:
                # TODO: implement answerY/N
                self.answers.append(self.answerYesNoQuestion(i))

    def printAnswers(self):
        for answer in self.answers:
            print(answer)

if __name__ == "__main__":
    # Files
    data_file = sys.argv[1]
    questions_file = sys.argv[2]

    # preprocess data
    data = preprocess.open_file(data_file)
    (preprocessed_data, tagged_data) = preprocess.pos_tokenize(data)
    vectors_data = preprocess.vectors(data)
    ner_data = preprocess.ner(data)

    # preprocess questions
    questions = preprocess.open_file(questions_file, question=True)
    (preprocessed_questions, tagged_questions) = preprocess.pos_tokenize(questions)
    vectors_questions = preprocess.vectors(questions)
    ner_questions = preprocess.ner(questions)

    # make the question answerer
    qAnswerer = Answerer(preprocessed_data, tagged_data, vectors_data, ner_data, preprocessed_questions, tagged_questions, vectors_questions, ner_questions)
    # answer questions
    qAnswerer.answerQuestions()
    # print results
    qAnswerer.printAnswers()